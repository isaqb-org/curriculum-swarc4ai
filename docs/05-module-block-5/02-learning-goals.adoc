=== {learning-goals}

// tag::DE[]


[[LZ-5-1]]
==== LZ 5-1: (Hardware)-Anforderungen an Training und Inferenz kennen

Die Teilnehmer:innen kennen unterschiedliche (Hardware)-Anforderungen für beispielsweise TPU, GPU oder CPU an Training und Inferenz.

[[LZ-5-2]]
==== LZ 5-2: Trade-Offs verschiedener Modellarchitekturen bezüglich der Qualitätsmerkmale kennen

Die Teilnehmer:innen können beispielhaft Trade-Offs verschiedener Modellarchitekturen bezüglich der Qualitätsmerkmale nennen. Insbesondere für Skalierung, Effizienz und Speicherlast sollten die Teilnehmer:innen die Trade-Offs sowie die Vor- und Nachteile von wichtigen Architekturen wie beispielsweise RNNs und Transformern kennen.


[[LZ-5-3]]
==== LZ 5-3: Verschiedene Qualitätsmerkmale eines ML-Modells abstimmen

Die Teilnehmer:innen kennen Möglichkeiten, um verschiedene Qualitätsmerkmale wie Genauigkeit, Effizienz und Speicherlast eines ML-Modells abzustimmen und gegeneinander
einzutauschen. Insbesondere kennen die Teilnehmer:innen die folgenden Techniken:

* Quantisierung
* Pruning
* Destillierung
* LoRA

[[LZ-5-4]]
==== LZ 5-4: Kosten, Stromverbrauch und nachhaltige Nutzung von KI (Green IT) verstehen

Die Teilnehmer:innen erlangen ein Verständnis für Kosten, Stromverbrauch und nachhaltige Nutzung von KI (Green IT). Insbesondere wissen die Teilnehmer:innen, wie man KI-Modelle und -Systeme entwickelt, die ressourcenschonend arbeiten, indem sie Speicher, Rechenleistung und Speicherplatz effizient nutzen.

[[LZ-5-5]]
==== LZ 5-5: MLOps für die Automatisierung des Life-Cycles eines Data-Science-Projekts überblicken

Die Teilnehmer:innen kennen den Begriff MLOps für die Automatisierung des Life-Cycles eines Data-Science-Projekts sowie den Zusammenhang mit DevOps.

[[LZ-5-6]]
==== LZ 5-6: Modelltraining, Parameter, Metriken und Ergebnisse tracken

Die Teilnehmer:innen haben ein Verständnis vom Tracking im Modelltraining, Parametern, Metriken und Ergebnissen.

[[LZ-5-7]]
==== LZ 5-7: ML-Modelle und darauf aufbauenden KI-Systeme evaluieren

Die Teilnehmer:innen überblicken Ansätze zur Evaluation von ML-Modellen und darauf aufbauenden KI-Systemen.

[[LZ-5-8]]
==== LZ 5-8: Arten von Drift sowie mögliche Ursachen und Lösungsansätze dafür kennen

Die Teilnehmer:innen kennen verschiedene Arten von Drift, wie beispielsweise Daten-Drift oder Modell-Drift, sowie mögliche Ursachen und Lösungsansätze dafür.

[[LZ-5-9]]
==== LZ 5-9: CI/CD-Pipelines, Modellmanagement und Deployment-Strategien für KI-Modelle überblicken

Die Teilnehmer:innen haben ein Verständnis von CI/CD-Pipelines, Modellmanagement und Deployment-Strategien für KI-Modelle.

[[LZ-5-10]]
==== LZ 5-10: Plattformen für die Modellbereitstellung kennen

Die Teilnehmer:innen kennen gängige Plattformen für die Modellbereitstellung, wie beispielsweise Huggingface Hub.

[[LZ-5-11]]
==== LZ 5-11: Werkzeuge für das Erstellen von POCs von KI-Systemen einordnen

Die Teilnehmer:innen können gängige Werkzeuge für das Erstellen von POCs von KI-Systemen, wie beispielsweise Gradio, nennen und verstehen die konzeptionelle Funktionsweise.


[[LZ-5-12]]
==== LZ 5-12: Deployment-Möglichkeiten von KI-Modellen kennen

Die Teilnehmer:innen kennen verschiedene Deployment-Möglichkeiten von KI-Modellen. Eine Auswahl der folgenden Möglichkeiten verstehen die Teilnehmer:innen:

* API Deployment
* Embedded Deployment
* Batch Prediction
* Streaming
* Containerization
* Serverless Deployment
* Cloud Services

[[LZ-5-13]]
==== LZ 5-13: Vor- und Nachteile von SaaS und Self-Hosting nennen

Die Teilnehmer:innen kennen die Vor- und Nachteile von SaaS und Self-Hosting und können dazwischen abwägen.

[[LZ-5-14]]
==== LZ 5-14: SaaS-KI-Lösungen überblicken

Die Teilnehmer:innen überblicken bekannte SaaS-KI-Lösungen, wie beispielsweise Azure OpenAI Services.

[[LZ-5-15]]
==== LZ 5-15: Embedded Deployments von ML-Modellen kennen

Die Teilnehmer:innen kennen verschiedene Möglichkeiten und Standards für Embedded Deployments von ML-Modellen.

[[LZ-5-16]]
==== LZ 5-16: Monitoring im Hinblick auf KI-spezifische Anforderungen verstehen

Die Teilnehmer:innen verstehen die Notwendigkeit für Monitoring, auch im Hinblick auf KI-spezifische Anforderungen wie das Tracking von Drift.

Die Teilnehmer:innen kennen relevante Metriken wie beispielsweise Accuracy, Precision, Recall, F1-Score, MAE, MSE, Perplexity, Latenz, Durchsatz und Ressourcenauslastung und verstehen, warum diese für das Monitoring relevant sind.

[[LZ-5-17]]
==== LZ 5-17: Beispiel-Werkzeuge für Monitoring überblicken

Die Teilnehmer:innen kennen Beispiel-Werkzeuge für Monitoring. Dies betrifft sowohl allgemeine Werkzeuge, wie beispielsweise Prometheus & Grafana,
als auch ML-spezifische wie beispielsweise MLflow.

[[LZ-5-18]]
==== LZ 5-18: Nutzer-Feedback sowie Methoden und Werkzeuge zur Sammlung von Nutzer-Feedback verstehen

Die Teilnehmer:innen verstehen den Nutzen von Nutzer-Feedback für das weitere Modelltraining. Darüber hinaus kennen die Teilnehmer:innen verschiedene Methoden und Werkzeuge zur Sammlung von Nutzer-Feedback, wie beispielsweise die Auswahl zwischen mehreren Antworten und Flagging in Gradio.


[[LZ-5-19]]
==== LZ 5-19: Methoden zur Nutzung von Feedback für das Modell-Training kennen

Die Teilnehmer:innen kennen verschiedene Methoden zur Nutzung von Feedback für das Modell-Training, wie beispielsweise RLHF, RLAIF und DPO.

[[LZ-5-20]]
==== LZ 5-20: [OPTIONAL] MLOps-Pipeline an einem Praxisbeispiel verstehen

Die Teilnehmer:innen erfahren anhand eines Praxisbeispiels, wie eine MLOps-Pipeline aussehen kann und welche Einsichten diese auf die Parameter, Metriken usw. bietet.

[[LZ-5-21]]
==== LZ 5-21: [OPTIONAL] Build vs. Buy Entscheidungen für MLOps Systeme/Komponenten treffen

Die Teilnehmer:innen sind in der Lage, Build vs. Buy Entscheidungen für MLOps Systeme/Komponenten zu treffen.

[[LZ-5-22]]
==== LZ 5-22: [OPTIONAL] MLOps-Werkzeuge und End-to-End Plattformen kennen

Die Teilnehmer:innen kennen bekannte MLOps-Werkzeuge und End-to-End Plattformen,wie beispielsweise:

* Domino Data Lab, h2o.ai, DVC, activeloop, aporia, argo, arize, bentoML, comet ML, DagsHub, Databricks MLOps Stacks, Feast, Kedro, Kubeflow, Metaflow, MLflow, MLRun, prefect, PrimeHub, Weights & Biases, WhyLabs, zenML, KNIME, RapidMiner, NVIDIA AI Enterprise, watsonx.ai
* OpenSource: MLFlow, Weights & Biases, ClearML
* PaaS: AWS SageMaker, Azure ML.

// end::DE[]






// tag::EN[]
[[LG-5-1]]
==== LG 5-1: Know (hardware) requirements for training and inference

The participants know the different (hardware) requirements for TPU, GPU or CPU for training and inference, for example.

[[LG-5-2]]
==== LG 5-2: Know the trade-offs of different model architectures with regard to quality characteristics
Participants will be able to name examples of trade-offs between different model architectures with regard to quality characteristics. In particular for scaling, efficiency and storage load, participants should know the trade-offs as well as the advantages and disadvantages of important architectures such as RNNs and transformers.

[[LG-5-3]]
==== LG 5-3: Adjust different quality features of an ML model

Participants will be familiar with ways to adjust various quality characteristics such as accuracy, efficiency and storage load of an ML model and to trade them off against each other. In particular, the participants know the following techniques:

- Quantization
- Pruning
- Distillation
- LoRA



[[LG-5-4]]
==== LG 5-4: Understanding the costs, power consumption and sustainable use of AI (Green IT)

Participants gain an understanding of the costs, power consumption and sustainable use of AI (green IT). In particular, participants will know how to develop AI models and systems that work in a resource-efficient manner by using memory, computing power and storage space efficiently.


[[LG-5-5]]
==== LG 5-5: Know (hardware) requirements for training and inference

The participants know the term MLOps for the automation of the life cycle of a data science project and the connection with DevOps.


[[LG-5-6]]
==== LG 5-6: Model training, parameters, metrics and results tracking

The participants have an understanding of tracking in model training, parameters, metrics and results.


[[LG-5-7]]
==== LG 5-7: Evaluate ML models and AI systems based on them

Participants will gain an overview of approaches for evaluating ML models and AI systems based on them.


[[LG-5-8]]
==== LG 5-8: Know types of drift and possible causes and solutions for them

Participants are familiar with different types of drift, such as data drift or model drift, as well as possible causes and solutions.


[[LG-5-9]]
==== LG 5-9: Overview of CI/CD pipelines, model management and deployment strategies for AI models

Participants will have an understanding of CI/CD pipelines, model management and deployment strategies for AI models.


[[LG-5-10]]
==== LG 5-10: Know platforms for model provision

The participants are familiar with common platforms for model provision, such as Huggingface Hub.


[[LG-5-11]]
==== LG 5-11: Classify tools for the creation of POCs of AI systems

Participants will be able to name common tools for creating POCs for AI systems, such as Gradio, and understand how they work conceptually.

[[LG-5-12]]
==== LG 5-12: Knowing the deployment options of AI models

Participants are familiar with various deployment options for AI models. Participants will understand a selection of the following options:

- API Deployment
- Embedded Deployment
- Batch Prediction
- Streaming
- Containerization
- Serverless Deployment
- Cloud services



[[LG-5-13]]
==== LG 5-13: List advantages and disadvantages of SaaS and self-hosting

The participants know the advantages and disadvantages of SaaS and self-hosting and can weigh up the pros and cons.


[[LG-5-14]]
==== LG 5-14: Overview of SaaS AI solutions

Participants will gain an overview of well-known SaaS AI solutions, such as Azure OpenAI Services.


[[LG-5-15]]
==== LG 5-15: Know embedded deployments of ML models

Participants will be familiar with various options and standards for embedded deployments of ML models.


[[LG-5-16]]
==== LG 5-16: Understanding monitoring with regard to AI-specific requirements

The participants understand the need for monitoring, also with regard to AI-specific requirements such as drift tracking.
Participants are familiar with relevant metrics such as accuracy, precision, recall and F1 score, MAE, MSE, perplexity, latency, throughput and resource utilization and understand why these are relevant for monitoring.

[[LG-5-17]]
==== LG 5-17: Overview of sample tools for monitoring

The participants know example tools for monitoring. This includes both general tools, such as Prometheus & Grafana, as well as ML-specific tools, such as MLflow.


[[LG-5-18]]
==== LG 5-18: Understanding user feedback and methods and tools for collecting user feedback

Participants understand the benefits of user feedback for further model training. In addition, participants are familiar with various methods and tools for collecting user feedback, such as choosing between multiple answers and flagging in Gradio.

[[LG-5-19]]
==== LG 5-19: Know methods for using feedback for model training

Participants are familiar with various methods of using feedback for model training, such as RLHF, RLAIF and DPO.


[[LG-5-20]]
==== LG 5-20: [OPTIONAL] Understanding the MLOps pipeline using a practical example

Using a practical example, participants will learn what an MLOps pipeline can look like and what insights it offers into parameters, metrics, etc.


[[LG-5-21]]
==== LG 5-21: [OPTIONAL] Make build vs. buy decisions for MLOps systems/components

Participants will be able to make build vs. buy decisions for MLOps systems/components.


[[LG-5-22]]
==== LG 5-22: [OPTIONAL] Know MLOps tools and end-to-end platforms

The participants are familiar with well-known MLOps tools and end-to-end platforms, such as:

- Domino Data Lab, h2o.ai, DVC, activeloop, aporia, argo, arize, bentoML, comet ML, DagsHub, Databricks MLOps Stacks, Feast, Kedro, Kubeflow, Metaflow, MLflow, MLRun, prefect, PrimeHub, Weights & Biases, WhyLabs, zenML, KNIME, RapidMiner, NVIDIA AI Enterprise, watsonx.ai
- OpenSource: MLFlow, Weights & Biases, ClearML
- PaaS: AWS SageMaker, Azure ML.



// end::EN[]
