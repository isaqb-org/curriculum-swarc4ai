=== {learning-goals}

// tag::DE[]


[[LZ-5-1]]
==== LZ 5-1: (Hardware)-Anforderungen an Training und Inferenz kennen

Die Teilnehmer:innen kennen unterschiedliche (Hardware)-Anforderungen für beispielsweise TPU, GPU oder CPU an Training und Inferenz.

[[LZ-5-2]]
==== LZ 5-2: Trade-Offs verschiedener Modellarchitekturen bezüglich der Qualitätsmerkmale kennen

Die Teilnehmer:innen können beispielhaft Trade-Offs verschiedener Modellarchitekturen bezüglich der Qualitätsmerkmale nennen. Insbesondere für Skalierung, Effizienz und Speicherlast sollten die Teilnehmer:innen die Trade-Offs sowie die Vor- und Nachteile von wichtigen Architekturen wie beispielsweise RNNs und Transformern kennen.


[[LZ-5-3]]
==== LZ 5-3: Verschiedene Qualitätsmerkmale eines ML-Modells abstimmen

Die Teilnehmer:innen kennen Möglichkeiten, um verschiedene Qualitätsmerkmale wie Genauigkeit, Effizienz und Speicherlast eines ML-Modells abzustimmen und gegeneinander
einzutauschen. Insbesondere kennen die Teilnehmer:innen die folgenden Techniken:

* Quantisierung
* Pruning
* Destillierung
* LoRA

[[LZ-5-4]]
==== LZ 5-4: Kosten, Stromverbrauch und nachhaltige Nutzung von KI (Green IT) verstehen

Die Teilnehmer:innen erlangen ein Verständnis für Kosten, Stromverbrauch und nachhaltige Nutzung von KI (Green IT). Insbesondere wissen die Teilnehmer:innen, wie man KI-Modelle und -Systeme entwickelt, die ressourcenschonend arbeiten, indem sie Speicher, Rechenleistung und Speicherplatz effizient nutzen.

[[LZ-5-5]]
==== LZ 5-5: MLOps für die Automatisierung des Life-Cycles eines Data-Science-Projekts überblicken

Die Teilnehmer:innen kennen den Begriff MLOps für die Automatisierung des Life-Cycles eines Data-Science-Projekts sowie den Zusammenhang mit DevOps.

[[LZ-5-6]]
==== LZ 5-6: Modelltraining, Parameter, Metriken und Ergebnisse tracken

Die Teilnehmer:innen haben ein Verständnis vom Tracking im Modelltraining, Parametern, Metriken und Ergebnissen.

[[LZ-5-7]]
==== LZ 5-7: ML-Modelle und darauf aufbauenden KI-Systeme evaluieren

Die Teilnehmer:innen überblicken Ansätze zur Evaluation von ML-Modellen und darauf aufbauenden KI-Systemen.

[[LZ-5-8]]
==== LZ 5-8: Arten von Drift sowie mögliche Ursachen und Lösungsansätze dafür kennen

Die Teilnehmer:innen kennen verschiedene Arten von Drift, wie beispielsweise Daten-Drift oder Modell-Drift, sowie mögliche Ursachen und Lösungsansätze dafür.

[[LZ-5-9]]
==== LZ 5-9: CI/CD-Pipelines, Modellmanagement und Deployment-Strategien für KI-Modelle überblicken

Die Teilnehmer:innen haben ein Verständnis von CI/CD-Pipelines, Modellmanagement und Deployment-Strategien für KI-Modelle.

[[LZ-5-10]]
==== LZ 5-10: Plattformen für die Modellbereitstellung kennen

Die Teilnehmer:innen kennen gängige Plattformen für die Modellbereitstellung, wie beispielsweise Huggingface Hub.

[[LZ-5-11]]
==== LZ 5-11: Werkzeuge für das Erstellen von POCs von KI-Systemen einordnen

Die Teilnehmer:innen können gängige Werkzeuge für das Erstellen von POCs von KI-Systemen, wie beispielsweise Gradio, nennen und verstehen die konzeptionelle Funktionsweise.


[[LZ-5-12]]
==== LZ 5-12: Deployment-Möglichkeiten von KI-Modellen kennen

Die Teilnehmer:innen kennen verschiedene Deployment-Möglichkeiten von KI-Modellen. Eine Auswahl der folgenden Möglichkeiten verstehen die Teilnehmer:innen:

* API Deployment
* Embedded Deployment
* Batch Prediction
* Streaming
* Containerization
* Serverless Deployment
* Cloud Services

[[LZ-5-13]]
==== LZ 5-13: Vor- und Nachteile von SaaS und Self-Hosting nennen

Die Teilnehmer:innen kennen die Vor- und Nachteile von SaaS und Self-Hosting und können dazwischen abwägen.

[[LZ-5-14]]
==== LZ 5-14: SaaS-KI-Lösungen überblicken

Die Teilnehmer:innen überblicken bekannte SaaS-KI-Lösungen, wie beispielsweise Azure OpenAI Services.

[[LZ-5-15]]
==== LZ 5-15: Embedded Deployments von ML-Modellen kennen

Die Teilnehmer:innen kennen verschiedene Möglichkeiten und Standards für Embedded Deployments von ML-Modellen.

[[LZ-5-16]]
==== LZ 5-16: Monitoring im Hinblick auf KI-spezifische Anforderungen verstehen

Die Teilnehmer:innen verstehen die Notwendigkeit für Monitoring, auch im Hinblick auf KI-spezifische Anforderungen wie das Tracking von Drift.

Die Teilnehmer:innen kennen relevante Metriken wie beispielsweise Accuracy, Precision, Recall, F1-Score, MAE, MSE, Perplexity, Latenz, Durchsatz und Ressourcenauslastung und verstehen, warum diese für das Monitoring relevant sind.

[[LZ-5-17]]
==== LZ 5-17: Beispiel-Werkzeuge für Monitoring überblicken

Die Teilnehmer:innen kennen Beispiel-Werkzeuge für Monitoring. Dies betrifft sowohl allgemeine Werkzeuge, wie beispielsweise Prometheus & Grafana,
als auch ML-spezifische wie beispielsweise MLflow.

[[LZ-5-18]]
==== LZ 5-18: Nutzer-Feedback sowie Methoden und Werkzeuge zur Sammlung von Nutzer-Feedback verstehen

Die Teilnehmer:innen verstehen den Nutzen von Nutzer-Feedback für das weitere Modelltraining. Darüber hinaus kennen die Teilnehmer:innen verschiedene Methoden und Werkzeuge zur Sammlung von Nutzer-Feedback, wie beispielsweise die Auswahl zwischen mehreren Antworten und Flagging in Gradio.


[[LZ-5-19]]
==== LZ 5-19: Methoden zur Nutzung von Feedback für das Modell-Training kennen

Die Teilnehmer:innen kennen verschiedene Methoden zur Nutzung von Feedback für das Modell-Training, wie beispielsweise RLHF, RLAIF und DPO.

[[LZ-5-20]]
==== LZ 5-20: [OPTIONAL] MLOps-Pipeline an einem Praxisbeispiel verstehen

Die Teilnehmer:innen erfahren anhand eines Praxisbeispiels, wie eine MLOps-Pipeline aussehen kann und welche Einsichten diese auf die Parameter, Metriken usw. bietet.

[[LZ-5-21]]
==== LZ 5-21: [OPTIONAL] Build vs. Buy Entscheidungen für MLOps Systeme/Komponenten treffen

Die Teilnehmer:innen sind in der Lage, Build vs. Buy Entscheidungen für MLOps Systeme/Komponenten zu treffen.

[[LZ-5-22]]
==== LZ 5-22: [OPTIONAL] MLOps-Werkzeuge und End-to-End Plattformen kennen

Die Teilnehmer:innen kennen bekannte MLOps-Werkzeuge und End-to-End Plattformen,wie beispielsweise:

* Domino Data Lab, h2o.ai, DVC, activeloop, aporia, argo, arize, bentoML, comet ML, DagsHub, Databricks MLOps Stacks, Feast, Kedro, Kubeflow, Metaflow, MLflow, MLRun, prefect, PrimeHub, Weights & Biases, WhyLabs, zenML, KNIME, RapidMiner, NVIDIA AI Enterprise, watsonx.ai
* OpenSource: MLFlow, Weights & Biases, ClearML
* PaaS: AWS SageMaker, Azure ML.

// end::DE[]

// tag::EN[]
[[LG-5-1]]
==== LG 5-1: Know the (hardware) requirements for training and inference

The participants know the different (hardware) requirements for training and inference, for example TPU, GPU or CPU.

[[LG-5-2]]
==== LG 5-2: Know the Trade-offs of different model architectures with regard to quality characteristics

The participants can give examples of trade-offs of different model architectures with regard to quality characteristics. In particular, for scaling, efficiency and memory load, the participants should know the trade-offs as well as the advantages and disadvantages of important architectures such as RNNs and transformers.

[[LG-5-3]]
==== LG 5-3: Coordinate different quality characteristics of an ML-Model

The participants know ways to coordinate different quality characteristics such as accuracy, efficiency and memory load of an ML-Model and to exchange them for each other. In particular, participants are familiar with the following techniques:

* Quantization
* Pruning
* Distillation
* LoRA (Low Rank Adaptation)

[[LG-5-4]]
==== LG 5-4: Understanding Costs, Power consumption and sustainable use of AI (Green IT)

Participants gain an understanding of costs, power consumption and sustainable use of AI (Green IT). In particular, participants know how to develop AI-Models and systems that work in a resource-saving manner by using memory, computing power and storage space efficiently.

[[LG-5-5]]
==== LG 5-5: Overview of MLOps for automating the life cycle of a Data-Science project

Participants are familiar with the term MLOps for automating the life cycle of a data science project and the connection with DevOps.

[[LG-5-6]]
==== LG 5-6: Tracking model training, parameters, metrics and results

The participants have an understanding of tracking in model training, parameters, metrics and results.

[[LG-5-7]]
==== LG 5-7: Evaluating ML-Models and AI-Systems based on them

The participants have an overview of approaches to evaluating ML-Models and AI-Systems based on them.

[[LG-5-8]]
==== LG 5-8: Knowing types of drift as well as possible causes and solutions for them

The participants know different types of drift, such as data-drift or model-drift, as well as possible causes and solutions for them.

[[LG-5-9]]
==== LG 5-9: Overview of CI/CD pipelines, model management and deployment strategies for AI-Models

The participants have an understanding of CI/CD pipelines, model management and deployment strategies for AI-Models.

[[LG-5-10]]
==== LG 5-10: Know platforms for model deployment

The participants are familiar with common platforms for model deployment, such as Huggingface Hub.

[[LG-5-11]]
==== LG 5-11: Classify tools for creating PoCs of AI-Systems

The participants can name common tools for creating POCs of AI systems, such as Gradio, and understand how they work conceptually.

[[LG-5-12]]
==== LG 5-12: Know the deployment options for AI-Models

The participants know various deployment options for AI-Models. The participants understand a selection of the following options:

* API Deployment
* Embedded Deployment
* Batch Prediction
* Streaming
* Containerization
* Serverless Deployment
* Cloud Services

[[LG-5-13]]
==== LG 5-13: Name the advantages and disadvantages of SaaS and Self-hosting

The participants know the advantages and disadvantages of SaaS and Self-hosting and can weigh up the two.

[[LG-5-14]]
==== LG 5-14: Overview of SaaS AI-Solutions

The participants have an overview of well-known SaaS AI-Solutions, such as Azure OpenAI Services.

[[LG-5-15]]
==== LG-5-15: Know embedded deployments of ML-Models

The participants know various options and standards for embedded deployments of ML models.

[[LG-5-16]]
==== LG 5-16: Understand monitoring with regard to AI-specific requirements

The participants understand the need for monitoring, also with regard to AI-specific requirements such as tracking drift.

The participants know relevant metrics such as accuracy, precision, recall, F1 score, MAE, MSE, perplexity, latency, throughput and resource utilization and understand why these are relevant for monitoring.

[[LG-5-17]]
==== LG 5-17: Overview of example tools for monitoring

The participants know example tools for monitoring. This applies to both general tools, such as Prometheus & Grafana,
as well as ML-specific ones such as MLflow.

[[LG-5-18]]
==== LG 5-18: Understanding user feedback and methods and tools for collecting user feedback

The participants understand the benefits of user feedback for further model training. In addition, the participants know various methods and tools for collecting user feedback,such as choosing between multiple answers and flagging in Gradio.

[[LG-5-19]]
==== LG 5-19: Know methods for using feedback for model training

The participants know various methods for using feedback for model training, such as RLHF, RLAIF and DPO.

[[LG-5-20]]
==== LG 5-20: [OPTIONAL] Understanding the MLOps pipeline using a practical example

The participants learn what an MLOps pipeline can look like and what insights it offers on the parameters, metrics, etc. using a practical example.

[[LG-5-21]]
==== LG 5-21: [OPTIONAL] Making build vs. buy decisions for MLOps Systems/Components

The participants are able to make build vs. buy decisions for MLOps Systems/Components.

[[LG-5-22]]
==== LG 5-22: [OPTIONAL] Know MLOps tools and end-to-end platforms

The participants know well-known MLOps tools and end-to-end platforms, such as:

* Domino Data Lab, h2o.ai, DVC, activeloop, aporia, argo, arize, bentoML, comet ML, DagsHub, Databricks MLOps Stacks, Feast, Kedro, Kubeflow, Metaflow, MLflow, MLRun, prefect, PrimeHub, Weights & Biases, WhyLabs, zenML, KNIME, RapidMiner, NVIDIA AI Enterprise, watsonx.ai
* OpenSource: MLFlow, Weights & Biases, ClearML
* PaaS: AWS SageMaker, Azure ML.


// end::EN[]
